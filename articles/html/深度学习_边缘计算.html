<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh" xml:lang="zh">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>深度学习:边缘计算</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="icon" href="../../imgs/favicon.ico" type="image/x-icon"/> <link rel="shortcut icon" href="../../imgs/favicon.ico" type="image/x-icon" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-140731880-1"></script>
  <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date());
  gtag('config', 'UA-140731880-1'); </script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">深度学习:边缘计算</h1>
<p class="date">2018-10-08 21:13:04</p>
</header>
<nav id="TOC">
<ul>
<li><a href="#前言">前言</a><ul>
<li><a href="#asic-是未来发展方向"><span>ASIC 是未来发展方向</span></a></li>
<li><a href="#性能指标">性能指标</a></li>
</ul></li>
<li><a href="#模型推理">模型推理</a><ul>
<li><a href="#tensorrt">tensorRT</a></li>
</ul></li>
</ul>
</nav>
<h1 id="前言">前言</h1>
<h2 id="asic-是未来发展方向"><a href="https://baijiahao.baidu.com/s?id=1610466874421673569&amp;wfr=spider&amp;for=pc">ASIC 是未来发展方向</a></h2>
<p>因下面两个趋势的消失，通用计算芯片的发展已趋于平缓，专用计算芯片开始进入大众视野：</p>
<ol type="1">
<li>摩尔定律：当价格不变时，集成电路上可容纳的元器件的数目，约每隔18-24个月便会增加一倍，性能也将提升一倍。</li>
<li><a href="http://www.newsmth.net/nForum/#!article/CSArch/43360">Dennard Scaling</a> ：晶体管尺寸变小，功耗会同比变小，换句话说相同面积下功耗不变。</li>
</ol>
<h2 id="性能指标">性能指标</h2>
<ol type="1">
<li>所需计算量（FLOPs, float-point operations, 浮点运算次数）：比如对于 1 * 1 的卷积操作，当输入输出通道数分别为 c1, c2 时，FLOPs = c1 * h * w * c2；</li>
<li>所需带宽量（MAC, memory access cost）：比如对于 1 * 1 的卷积操作，当输入输出通道数分别为 c1, c2 时，MAC = h * w * c1 + c1 * c2 + h * w * c2；</li>
<li>数据吞吐量（Throughput）：单位时间内可以成功传输的数据数量；</li>
<li>IOPS：单位时间内系统能处理的I/O请求数量，一般以每秒处理的I/O请求次数为单位，用于衡量磁盘性能；</li>
<li><a href="https://community.emc.com/docs/DOC-28653">带宽（band width）：单位时间内可以传输的数据数量，即在传输管道中可以传递数据的能力</a>；
<ul>
<li>带宽为吞吐量的理想值，因为受各种低效率的影响，吞吐量常比带宽低很多</li>
<li>带宽为链路上的可用带宽，只取决于链路时钟和信道编码，吞吐量为实际链路中每秒所能传送的比特数，为实际的测试性能</li>
<li>性能监控工具显示 IOPS 低或者 Throughput 低于预期，先不要直接认为存储性能存在问题，搞清楚应用的 I/O 大小，再做后续判断</li>
</ul></li>
</ol>
<h1 id="模型推理">模型推理</h1>
<h2 id="tensorrt">tensorRT</h2>
<h3 id="优化">优化</h3>
<h4 id="layer-tensor-fusion">layer &amp; tensor fusion</h4>
<p>在部署模型推理时，这每一层的运算操作都是由GPU完成的，具体是GPU通过启动不同的CUDA（Compute unified device architecture）核心来完成计算的，CUDA核心计算张量的速度是很快的，但是往往大量的时间是浪费在CUDA核心的启动和对每一层输入/输出张量的读写操作上面，这造成了内存带宽的瓶颈和GPU资源的浪费。</p>
<p>故可通过合并计算图来优化推理：横向合并可以把卷积、偏置和激活层合并成一层CBR(conv+bias+relu)结构，只占用一个CUDA核心；纵向合并可以把结构相同，但是权值不同的层合并成一个更宽的层（多层输出的结构可借用该方法做输出层的解析加速，如下图所示），也只占用一个CUDA核心。</p>
<figure>
<img src="../images/dl/object%20detection/YOLOv3trt.jpg" alt="YOLOv3trt" /><figcaption>YOLOv3trt</figcaption>
</figure>
<h4 id="weight-activation-precision-calibration"><a href="https://arleyzhang.github.io/articles/923e2c40/">weight &amp; activation precision calibration</a></h4>
<h4 id="others">others</h4>
<ul>
<li>kernel auto-tuning: TensorRT will pick the implementation from a library of kernels that delivers the best performance for the target GPU, input data size, filter size, tensor layout, batch size and other parameters.</li>
<li>dynamic tensor memory: 在每个tensor的使用期间，TensorRT会为其指定显存，避免显存重复申请，减少内存占用和提高重复使用效率。</li>
<li>multi-stream execution: Scalable design to process multiple input streams in parallel.</li>
</ul>
<h3 id="使用流程">使用流程</h3>
<h4 id="build">build</h4>
<p>build: Import and optimize trained models to generate inference engines.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp"><code class="sourceCode cpp"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co">// 创建一个builder</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2">IBuilder* builder = createInferBuilder(<span class="va">logger_</span>);</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co">// 创建一个network对象，不过这时network对象只是一个空架子</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">INetworkDefinition* network = builder-&gt;createNetwork();</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">ICaffeParser *parser = createCaffeParser();</a>
<a class="sourceLine" id="cb1-6" data-line-number="6"><span class="co">// 这一步之后network对象里面的参数才被填充，才具有实际的意义</span></a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="kw">auto</span> blobNameToTensor = parser-&gt;parse(deployFile,                                                 modelFile,</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">                                      *network,</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">                                      DataType::kFLOAT);</a>
<a class="sourceLine" id="cb1-10" data-line-number="10"><span class="cf">for</span> (<span class="kw">auto</span>&amp; s : <span class="va">vOutputBlobNames_</span>)</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">  network-&gt;markOutput(*blobNameToTensor-&gt;find(s.c_str()));</a>
<a class="sourceLine" id="cb1-12" data-line-number="12"><span class="co">// 设置batchsize和工作空间，然后创建inference engine</span></a>
<a class="sourceLine" id="cb1-13" data-line-number="13">builder-&gt;setMaxBatchSize(<span class="va">maxBatchSize_</span>);</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">builder-&gt;setMaxWorkspaceSize(<span class="dv">16</span> &lt;&lt; <span class="dv">20</span>); </a>
<a class="sourceLine" id="cb1-15" data-line-number="15"><span class="co">// 调用buildCudaEngine时才会进行前述的层间融合或精度校准优化方式</span></a>
<a class="sourceLine" id="cb1-16" data-line-number="16">ICudaEngine* <span class="va">pEngine_</span> = builder-&gt;buildCudaEngine(*network);</a>
<a class="sourceLine" id="cb1-17" data-line-number="17">network-&gt;destroy(); builder-&gt;destroy();</a></code></pre></div>
<p>解析caffe模型之后，必须要指定输出tensor，设置batchsize和工作空间。设置batchsize就跟使用caffe测试是一样的，设置工作空间是进行前述层间融合和张量融合的必要措施。层间融合和张量融合的过程是在调用<code>builder-&gt;buildCudaEngine</code>时才进行的。</p>
<h4 id="deployment">deployment</h4>
<p>deploy: Generate runtime inference engine for inference.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp"><code class="sourceCode cpp"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co">// 创建上下文环境 context，用于启动kernel</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2">IExecutionContext *context = <span class="va">pEngine_</span>-&gt;createExecutionContext();</a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co">//获取输入，输出tensor索引</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4"><span class="dt">int</span> inputIndex = <span class="va">pEngine_</span>-&gt;getBindingIndex(inputLayerName),</a>
<a class="sourceLine" id="cb2-5" data-line-number="5"><span class="dt">int</span> outputIndex = <span class="va">pEngine_</span>-&gt;getBindingIndex(outLayerName);</a>
<a class="sourceLine" id="cb2-6" data-line-number="6"><span class="co">// Allocate GPU memory for Input / Output data</span></a>
<a class="sourceLine" id="cb2-7" data-line-number="7"><span class="dt">void</span>* buffers = malloc(<span class="va">pEngine_</span>-&gt;getNbBindings() * <span class="kw">sizeof</span>(<span class="dt">void</span>*));</a>
<a class="sourceLine" id="cb2-8" data-line-number="8">cudaMalloc(&amp;buffers[inputIndex], batchSize * size_of_single_input);</a>
<a class="sourceLine" id="cb2-9" data-line-number="9">cudaMalloc(&amp;buffers[outputIndex], batchSize * size_of_single_output);</a>
<a class="sourceLine" id="cb2-10" data-line-number="10"><span class="co">//使用cuda 流来管理并行计算</span></a>
<a class="sourceLine" id="cb2-11" data-line-number="11"><span class="dt">cudaStream_t</span> stream;</a>
<a class="sourceLine" id="cb2-12" data-line-number="12">cudaStreamCreate(&amp;stream);</a>
<a class="sourceLine" id="cb2-13" data-line-number="13"><span class="co">//从内存到显存，input是读入内存中的数据；buffers[inputIndex]是显存上的存储区域，用于存放输入数据</span></a>
<a class="sourceLine" id="cb2-14" data-line-number="14">cudaMemcpyAsync(buffers[inputIndex], </a>
<a class="sourceLine" id="cb2-15" data-line-number="15">                input, </a>
<a class="sourceLine" id="cb2-16" data-line-number="16">                batchSize * size_of_single_input, </a>
<a class="sourceLine" id="cb2-17" data-line-number="17">                cudaMemcpyHostToDevice, </a>
<a class="sourceLine" id="cb2-18" data-line-number="18">                stream);</a>
<a class="sourceLine" id="cb2-19" data-line-number="19"><span class="co">//启动cuda核计算</span></a>
<a class="sourceLine" id="cb2-20" data-line-number="20">context.enqueue(batchSize, buffers, stream, <span class="kw">nullptr</span>);</a>
<a class="sourceLine" id="cb2-21" data-line-number="21"><span class="co">//从显存到内存，buffers[outputIndex]是显存中的存储区，存放模型输出；output是内存中的数据</span></a>
<a class="sourceLine" id="cb2-22" data-line-number="22">cudaMemcpyAsync(output, </a>
<a class="sourceLine" id="cb2-23" data-line-number="23">                buffers[outputIndex], </a>
<a class="sourceLine" id="cb2-24" data-line-number="24">                batchSize * size_of_single_output, </a>
<a class="sourceLine" id="cb2-25" data-line-number="25">                cudaMemcpyDeviceToHost, </a>
<a class="sourceLine" id="cb2-26" data-line-number="26">                stream));</a>
<a class="sourceLine" id="cb2-27" data-line-number="27"><span class="co">//如果使用了多个cuda流，需要同步</span></a>
<a class="sourceLine" id="cb2-28" data-line-number="28">cudaStreamSynchronize(stream);</a>
<a class="sourceLine" id="cb2-29" data-line-number="29">cudaStreamDestroy(stream);</a>
<a class="sourceLine" id="cb2-30" data-line-number="30">cudaFree(buffers[inputIndex]);</a>
<a class="sourceLine" id="cb2-31" data-line-number="31">cudaFree(buffers[outputIndex]);</a></code></pre></div>
<p>deploy阶段主要完成推理过程，Kernel Auto-Tuning 和 Dynamic Tensor Memory 应该是在这里完成的。将上面一个步骤中的plan文件首先反序列化，并创建一个 runtime engine，然后就可以输入数据，然后输出分类向量结果或检测结果。</p>
<a style="color:black;font-size:1em;float:right;margin-right:30px;margin-bottom:40px;" href="../../index.html">[Return to the homepage]</a>
</body>
</html>
