<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh" xml:lang="zh">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>深度学习_基本件</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="style.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="icon" href="../../imgs/favicon.ico" type="image/x-icon"/> <link rel="shortcut icon" href="../../imgs/favicon.ico" type="image/x-icon" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-140731880-1"></script>
  <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date());
  gtag('config', 'UA-140731880-1'); </script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">深度学习_基本件</h1>
<p class="date">2019-07-15 15:21:47</p>
</header>
<nav id="TOC">
<ul>
<li><a href="#attention"><span>Attention</span></a></li>
<li><a href="#损失函数">损失函数</a><ul>
<li><a href="#应对样本不均衡">应对样本不均衡</a></li>
</ul></li>
</ul>
</nav>
<h1 id="attention"><a href="https://xdaping.github.io/posts/nlp-self-attention-models.html">Attention</a></h1>
<figure>
<img src="../images/nlp/attention计算过程.jpg" alt="attention计算过程" /><figcaption>attention计算过程</figcaption>
</figure>
<p>其中query为目标语言端的h，key为源语言端的h，value也为源语言端的h，通过点积等方法计算相似度后得到该时刻t的a，将value与a做加权求和后得到attention value，即得到该t时刻目标语言端的h。</p>
<p>简单来说就是将当前时刻的目标语言与源语言的各个向量求个相似度，用来对源语言的各个向量做加权求和，从而通过attention机制实现源语言与目标语言的对齐。</p>
<h1 id="损失函数">损失函数</h1>
<h2 id="应对样本不均衡">应对样本不均衡</h2>
<h3 id="focal-loss">focal loss</h3>
<p><span class="math display">\[
loss = \{^{-\alpha (1-y&#39;)^\gamma \log y&#39;, y=1}
_{-(1-\alpha)(y&#39;)^\gamma \log (1-y&#39;), y=0}.
\]</span></p>
<ul>
<li>focal loss 用于在训练过程中提高难分样本的权重，降低易分样本的权重，即尽可能地将计算资源用在难分样本上，从而提高模型的收敛能力；或者可认为是 AdaBoost 在深度学习上的应用（resnet 是 GradientBoost 在深度学习上的应用）。</li>
<li>其中 <span class="math inline">\(\alpha\)</span> 用以平衡正负样本数据集本身的不平衡，与 focal loss 无关。</li>
<li><span class="math inline">\(\gamma\)</span> 表示对难分样本关注的程度。</li>
</ul>
<h4 id="应用">应用</h4>
<h5 id="一阶目标检测">一阶目标检测</h5>
<p>对于二阶目标检测算法，有特征重采样过程，可在该过程中通过固定正负样本比例或在线困难样本挖掘（OHEM）使得前景和背景相对平衡；而一阶目标检测算法最多只能在数据集上做固定正负样本比例的处理，训练会被大量易分样本主导，导致模型难以收敛至更优解，focal loss 用 Boost 的方法解决了这个问题。</p>
<a style="color:black;font-size:1em;float:right;margin-right:30px;margin-bottom:40px;" href="../../index.html">[Return to the homepage]</a>
</body>
</html>
